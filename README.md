# Vision.ai

**Vision.ai** is an AI-powered assistive technology designed for visually impaired individuals. It generates real-time captions for uploaded images using state-of-the-art Transformer architectures and converts them into speech, enabling users to understand their surroundings effortlessly.

## ğŸš€ Features

- **Real-Time Image Captioning**  
  Uses **CNN + Transformer architecture** for accurate and context-aware caption generation.

- **Speech Output (Text-to-Speech)**  
  Converts generated captions into speech using `pyttsx3` for enhanced accessibility.

- **AI-Driven Language Enhancement**  
  Natural Language Processing techniques refine the output for grammatical and contextual correctness.

- **User-Friendly Interface**  
  Simple, intuitive image upload and output interface designed for accessibility.

## ğŸ› ï¸ Tech Stack

- **Deep Learning**
  - CNN (Image Feature Extraction)
  - Transformer (Caption Generation)
  - GAN (Optional) for refinement/enhancement

- **Machine Learning**
  - Model tuning and caption optimization

- **Natural Language Processing (NLP)**
  - Grammar and context correction for fluent captions

- **Text-to-Speech (TTS)**
  - `pyttsx3` for speech synthesis

- **Frontend**
  - Streamlit-based User Interface

- **Backend**
  - Python (Modular Structure)

- **Libraries & Tools**
  - PyTorch, OpenCV, NumPy, scikit-learn, NLTK, torchvision, pyttsx3, transformers

## ğŸ“‚ Project Structure

\`\`\`
Vision.ai/
â”œâ”€â”€ Data/
â”‚   â””â”€â”€ captions/
â”‚   â””â”€â”€ images/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ captioning/
â”‚       â”œâ”€â”€ transforms.py
â”‚       â”œâ”€â”€ utils.py
â”‚       â”œâ”€â”€ vocabulary.py
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ test_transforms.py
â”‚   â”œâ”€â”€ test_utils.py
â”‚   â””â”€â”€ testVocab.py
â”œâ”€â”€ train/
â”‚   â””â”€â”€ train_captioning.py
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.py
â”‚   â””â”€â”€ logger.py
â”œâ”€â”€ models/
â”‚   â””â”€â”€ encoder_decoder.py
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ streamlit_app.py
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ Docs/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
\`\`\`

## âœ”ï¸ Completed Modules

- `transforms.py` â€“ Image preprocessing  
- `utils.py` â€“ Caption cleaning & loading  
- `vocabulary.py` â€“ Vocabulary creation, stoi/itos mapping, numericalization  
- `test_transforms.py` â€“ Unit tests for transforms  
- `test_utils.py` â€“ Unit tests for utils  
- `testVocab.py` â€“ Unit tests for vocabulary module

## ğŸ§ª Testing

All core modules are tested using `unittest`.

Run tests individually:
\`\`\`bash
python -m unittest test/test_transforms.py
python -m unittest test/test_utils.py
python -m unittest test/testVocab.py
\`\`\`

Or run all tests (optional if `pytest` installed):
\`\`\`bash
pytest test/
\`\`\`

## ğŸ“¦ Installation

1. Clone the repository:
\`\`\`bash
git clone https://github.com/nikhilitz/Vision.ai.git
cd Vision.ai
\`\`\`

2. Create and activate virtual environment:
\`\`\`bash
python -m venv imgcap
source imgcap/bin/activate  # On Windows: imgcap\Scripts\activate
\`\`\`

3. Install dependencies:
\`\`\`bash
pip install -r requirements.txt
\`\`\`

4. Download NLTK tokenizer data (once only):
\`\`\`bash
python -m nltk.downloader punkt
\`\`\`

## ğŸ“Š Upcoming Work

- `dataset.py` â€“ PyTorch Dataset for images + captions  
- `encoder_decoder.py` â€“ CNN + Transformer model  
- `train_captioning.py` â€“ Training loop  
- `evaluate.py` â€“ BLEU score evaluation  
- `inference.py` â€“ Caption generation from uploaded image  
- `tts_engine.py` â€“ Convert text to speech  
- `translator.py` â€“ Optional caption translation  
- `streamlit_app.py` â€“ Frontend UI  
- `Dockerfile` â€“ Containerization

## ğŸ“¬ Contact

**Author:** Nikhil Gupta  
GitHub: https://github.com/nikhilitz

## ğŸ“ License

MIT License â€“ feel free to use, contribute, or modify.
